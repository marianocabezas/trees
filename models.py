import itertools
import time
import numpy as np
import torch
from functools import partial
from torch import nn
import torch.nn.functional as F
from data_manipulation.models import BaseModel
from data_manipulation.utils import to_torch_var, time_to_string
from data_manipulation.criterions import flip_loss, focal_loss


def dsc_loss(pred, target, smooth=0.1):
    """
    Loss function based on a single class DSC metric.
    :param pred: Predicted values. This tensor should have the shape:
     [batch_size, data_shape]
    :param target: Ground truth values. This tensor can have multiple shapes:
     - [batch_size, n_classes, data_shape]: This is the expected output since
       it matches with the predicted tensor.
     - [batch_size, data_shape]: In this case, the tensor is labeled with
       values ranging from 0 to n_classes. We need to convert it to
       categorical.
    :param smooth: Parameter used to smooth the DSC when there are no positive
     samples.
    :return: The mean DSC for the batch
    """
    dims = pred.shape
    assert target.shape == pred.shape,\
        'Sizes between predicted and target do not match'
    target = target.type_as(pred)

    reduce_dims = tuple(range(1, len(dims)))
    num = (2 * torch.sum(pred * target, dim=reduce_dims))
    den = torch.sum(pred + target, dim=reduce_dims) + smooth
    dsc_k = num / den
    dsc = 1 - torch.mean(dsc_k)

    return torch.clamp(dsc, 0., 1.)


def positive_uncertainty_loss(
        pred, target, q, q_factor=0.5, base=F.binary_cross_entropy
):
    """
    Flip loss function for the positive labels based on:
    Richard McKinley, Michael Rebsamen, Raphael Meier, Mauricio Reyes,
    Christian Rummel and Roland Wiest. "Few-shot brain segmentation from weakly
    labeled data with deep heteroscedastic multi-task network".
    https://arxiv.org/abs/1904.02436
    The idea is to allow for mislabeling inside the labeled are (since the area
    of the annotation is arbitrary, but the location of it is right). Otherwise
    the background label is correct.
    :param pred: Predicted values. The shape of the tensor should be related
     to the base function.
    :param target: Ground truth values. The shape of the tensor should be
     related to the base function.
    :param q: Uncertainty output from the network. The shape of the tensor
     should be related to the base function.
    :param q_factor: Factor to normalise the value of q.
    :param base: Base function for the flip loss.
    :return: The flip loss given a base loss function
    """
    norm_q = q * q_factor
    z = (pred < 0.5).type_as(pred) * target
    q_target = (1 - target) * norm_q + target * (1 - norm_q)
    loss_seg = base(pred, q_target.type_as(pred).detach())
    loss_uncertainty = base(norm_q, z.detach())

    final_loss = loss_seg + loss_uncertainty

    return final_loss


class Autoencoder2D(BaseModel):
    def __init__(
            self,
            conv_filters,
            device=torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu"
            ),
            n_inputs=1,
            kernel_size=3,
            pooling=False,
            dropout=0,
    ):
        super().__init__()
        # Init
        self.pooling = pooling
        self.device = device
        self.dropout = dropout
        # Down path
        self.down = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(
                    f_in, f_out, kernel_size,
                    padding=kernel_size // 2,
                ),
                nn.ReLU(),
                nn.BatchNorm2d(f_out)
            ) for f_in, f_out in zip(
                [n_inputs] + conv_filters[:-2], conv_filters[:-1]
            )
        ])

        self.u = nn.Sequential(
            nn.Conv2d(
                conv_filters[-2], conv_filters[-1], kernel_size,
                padding=kernel_size // 2
            ),
            nn.ReLU(),
            nn.BatchNorm2d(conv_filters[-1])
        )

        # Up path
        down_out = conv_filters[-2::-1]
        up_out = conv_filters[:0:-1]
        deconv_in = map(sum, zip(down_out, up_out))
        self.up = nn.ModuleList([
            nn.Sequential(
                nn.ConvTranspose2d(
                    f_in, f_out, kernel_size,
                    padding=kernel_size // 2
                ),
                nn.ReLU(),
                nn.BatchNorm2d(f_out)
            ) for f_in, f_out in zip(
                deconv_in, down_out
            )
        ])

    def forward(self, input_s):
        down_inputs = []
        for c in self.down:
            c.to(self.device)
            input_s = F.dropout2d(
                c(input_s), self.dropout, self.training
            )
            down_inputs.append(input_s)
            if self.pooling:
                input_s = F.max_pool2d(input_s, 2)

        self.u.to(self.device)
        input_s = F.dropout2d(self.u(input_s), self.dropout, self.training)

        for d, i in zip(self.up, down_inputs[::-1]):
            d.to(self.device)
            if self.pooling:
                input_s = F.dropout2d(
                    d(
                        torch.cat(
                            (F.interpolate(input_s, size=i.size()[2:]), i),
                            dim=1
                        )
                    ),
                    self.dropout,
                    self.training
                )
            else:
                input_s = F.dropout2d(
                    d(torch.cat((input_s, i), dim=1)),
                    self.dropout,
                    self.training
                )

        return input_s


class Unet2D(BaseModel):
    def __init__(
            self,
            conv_filters=None,
            device=torch.device("cuda:0" if torch.cuda.is_available() else "cpu"),
            n_inputs=4, n_outputs=1
    ):
        super(Unet2D, self).__init__()
        # Init values
        if conv_filters is None:
            conv_filters = [32, 64, 128, 256]
        self.epoch = 0
        self.t_train = 0
        self.t_val = 0
        self.device = device

        # <Parameter setup>
        # I plan to change that to use the AutoEncoder framework in
        # data_manipulation.
        self.autoencoder = Autoencoder2D(
            conv_filters, device, n_inputs, pooling=True
        )

        # Deep supervision branch.
        # This branch adapts the bottleneck filters to work with the final
        # segmentation block.
        self.deep_seg = nn.Sequential(
            nn.Conv2d(conv_filters[-1], conv_filters[0], 1),
            nn.ReLU(),
            nn.BatchNorm2d(conv_filters[0]),
        )
        self.deep_seg.to(device)

        # Final segmentation block.
        self.seg = nn.Sequential(
            nn.Conv2d(conv_filters[0], conv_filters[0], 1),
            nn.ReLU(),
            nn.BatchNorm2d(conv_filters[0]),
            nn.Conv2d(conv_filters[0], n_outputs, 1)
        )
        self.seg.to(device)

        # Final uncertainty block.
        # For now, this block is only used on the main branch. I am not sure
        # on how useful it might be for the deep branch.
        self.unc = nn.Sequential(
            nn.Conv2d(conv_filters[0], conv_filters[0], 1),
            nn.ReLU(),
            nn.BatchNorm2d(conv_filters[0]),
            nn.Conv2d(conv_filters[0], n_outputs, 1)
        )
        self.unc.to(device)

        # <Loss function setup>
        self.train_functions = [
            # Focal loss for the main branch.
            {
                'name': 'xentr',
                'weight': 1,
                'f': lambda p, t: focal_loss(
                    torch.squeeze(p[0], dim=1),
                    torch.squeeze(t[0], dim=1).type_as(p[0]).to(p[0].device),
                    alpha=0.5
                )
            },
            # Focal loss for the deep supervision branch (bottleneck).
            {
                'name': 'dp xe',
                'weight': 1,
                'f': lambda p, t: focal_loss(
                    torch.squeeze(p[2], dim=1),
                    torch.squeeze(
                        F.max_pool2d(
                            t[0].type_as(p[2]),
                            2 ** len(self.autoencoder.down)),
                        dim=1
                    ).to(p[2].device),
                    alpha=0.5
                )
            },
            # DSC loss for the main branch.
            {
                'name': 'dsc',
                'weight': 1,
                'f': lambda p, t: dsc_loss(p[0], t[0])
            },
            # DSC loss for the deep supervision branch (bottleneck).
            {
                'name': 'dp dsc',
                'weight': 1,
                'f': lambda p, t: dsc_loss(
                    p[2],
                    F.max_pool2d(
                        t[0].type_as(p[2]),
                        2 ** len(self.autoencoder.down)
                    ).to(p[2].device)
                )
            },
            # Uncertainty loss based on the flip loss (by Mckinley et al).
            {
                'name': 'unc',
                'weight': 1,
                'f': lambda p, t: flip_loss(
                    torch.squeeze(p[0], dim=1),
                    torch.squeeze(t[0], dim=1).type_as(p[0]).to(p[0].device),
                    torch.squeeze(p[1], dim=1),
                    q_factor=1,
                    base=partial(focal_loss, alpha=0.5)
                )
            },
        ]
        self.val_functions = [
            # Focal loss for validation.
            # The weight is 0 because I feel like DSC is more important.
            {
                'name': 'xentr',
                'weight': 0,
                'f': lambda p, t: focal_loss(
                    torch.squeeze(p[0], dim=1),
                    torch.squeeze(t[0], dim=1).type_as(p[0]).to(p[0].device),
                    alpha=0.5
                )
            },
            # DSC loss for validation.
            {
                'name': 'dsc',
                'weight': 1,
                'f': lambda p, t: dsc_loss(p[0], t[0])
            },
            # Losses based on uncertainty values.for
            # Their weight is 0 because I don't want them to affect early
            # stopping. They are useful values to see how the uncertainty is
            # evolving, but that's it.
            {
                'name': 'μ unc',
                'weight': 0,
                'f': lambda p, t: torch.mean(p[1])
            },
            {
                'name': 'unc',
                'weight': 0,
                'f': lambda p, t: torch.min(p[1])
            }
        ]
        self.acc_functions = [
            # Max uncertainty.
            # I use it as an accuracy function, because we want to maximise
            # them (losses are minimised) and I actually think that the
            # maximum uncertainty should rise and the mininum fall for it
            # to be any useful.
            # Since accuracy metrics are there just for added information
            # and they should not affect early stopping, they do not have
            # weights.
            {
                'name': 'UNC',
                'f': lambda p, t: torch.max(p[1])
            },
        ]

        # <Optimizer setup>
        # We do this last step after all parameters are defined
        model_params = filter(lambda p: p.requires_grad, self.parameters())
        # self.optimizer_alg = torch.optim.Adadelta(model_params)
        self.optimizer_alg = torch.optim.Adam(model_params)
        # self.optimizer_alg = torch.optim.SGD(model_params, lr=1e-1)
        # self.autoencoder.dropout = 0.99
        # self.dropout = 0.99
        # self.ann_rate = 1e-2

    def forward(self, input_ae):
        input_s = self.autoencoder(input_ae)

        # Deep supervision.
        # We need to complete the down path and the bottleneck again.
        for c in self.autoencoder.down:
            input_ae = F.dropout2d(
                c(input_ae),
                self.autoencoder.dropout,
                self.autoencoder.training
            )
            input_ae = F.max_pool2d(input_ae, 2)
        input_ae = F.dropout2d(
            self.autoencoder.u(input_ae),
            self.autoencoder.dropout,
            self.autoencoder.training
        )

        # This is the last part of deep supervision
        input_ae = self.deep_seg(input_ae)

        # Since we are dealing with a binary problem, there is no need to use
        # softmax.
        multi_seg = torch.sigmoid(self.seg(input_s))
        unc = torch.sigmoid(self.unc(input_s))
        low_seg = torch.sigmoid(self.seg(input_ae))
        return multi_seg, unc, low_seg

    def dropout_update(self):
        super().dropout_update()
        self.autoencoder.dropout = self.dropout

    def test(
            self, data, patch_size=256, verbose=True
    ):
        # Init
        self.eval()
        seg = list()
        unc = list()

        # Init
        t_in = time.time()

        for i, im in enumerate(data):

            # Case init
            t_case_in = time.time()

            # This branch is only used when images are too big. In this case
            # they are split in patches and each patch is trained separately.
            # Currently, the image is partitioned in blocks with no overlap,
            # however, it might be a good idea to sample all possible patches,
            # test them, and average the results. I know both approaches
            # produce unwanted artifacts, so I don't know.
            if patch_size is not None:

                # Initial results. Filled to 0.
                seg_i = np.zeros(im.shape[1:])
                unc_i = np.zeros(im.shape[1:])
                tops_i = 0

                limits = tuple(
                    list(range(0, lim, patch_size))[:-1] + [lim - patch_size]
                    for lim in data.shape[1:]
                )
                limits_product = list(itertools.product(*limits))

                n_patches = len(limits_product)

                # The following code is just a normal test loop with all the
                # previously computed patches.
                for pi, (xi, xj) in enumerate(limits_product):
                    # Here we just take the current patch defined by its slice
                    # in the x and y axes. Then we convert it into a torch
                    # tensor for testing.
                    xslice = slice(xi, xi + patch_size)
                    yslice = slice(xj, xj + patch_size)
                    data_tensor = to_torch_var(
                        np.expand_dims(im[slice(None), xslice, yslice], axis=0)
                    )

                    # Testing itself.
                    with torch.no_grad():
                        torch.cuda.synchronize()
                        seg_pi, unc_pi, _ = self(data_tensor)
                        torch.cuda.synchronize()
                        torch.cuda.empty_cache()

                    # Then we just fill the results image.
                    seg_i[xslice, yslice] = np.squeeze(seg_pi.cpu().numpy())
                    unc_i[xslice, yslice] = np.squeeze(unc_pi.cpu().numpy())
                    tops_i += np.squeeze(tops_pi.cpu().numpy())

                    # Printing
                    init_c = '\033[0m' if self.training else '\033[38;5;238m'
                    whites = ' '.join([''] * 12)
                    percent = 20 * (pi + 1) // n_patches
                    progress_s = ''.join(['-'] * percent)
                    remainder_s = ''.join([' '] * (20 - percent))

                    t_out = time.time() - t_in
                    t_case_out = time.time() - t_case_in
                    time_s = time_to_string(t_out)

                    t_eta = (t_case_out / (pi + 1)) * (n_patches - (pi + 1))
                    eta_s = time_to_string(t_eta)
                    batch_s = '{:}Case {:03}/{:03} ({:03d}/{:03d})' \
                              ' [{:}>{:}] {:} ETA: {:}'.format(
                        init_c + whites, i + 1, len(data), pi + 1, n_patches,
                        progress_s, remainder_s, time_s, eta_s + '\033[0m'
                    )
                    print('\033[K', end='', flush=True)
                    print(batch_s, end='\r', flush=True)

            else:
                # If we use the whole image the process is way simpler.
                # We only need to convert the data into a torch tensor,
                # test it and return the results.
                data_tensor = to_torch_var(np.expand_dims(im, axis=0))

                # Testing
                with torch.no_grad():
                    torch.cuda.synchronize(self.device)
                    seg_pi, unc_pi, _ = self(data_tensor)
                    torch.cuda.synchronize(self.device)
                    torch.cuda.empty_cache()

                # Image squeezing.
                # The images have a batch number at the beginning. Since each
                # batch is just an image, that batch number is useless.
                seg_i = np.squeeze(seg_pi.cpu().numpy())
                unc_i = np.squeeze(unc_pi.cpu().numpy())

                # Printing
                init_c = '\033[0m' if self.training else '\033[38;5;238m'
                whites = ' '.join([''] * 12)
                percent = 20 * (i + 1)
                progress_s = ''.join(['-'] * percent)
                remainder_s = ''.join([' '] * (20 - percent))

                t_out = time.time() - t_in
                t_case_out = time.time() - t_case_in
                time_s = time_to_string(t_out)

                t_eta = (t_case_out / (i + 1)) * (len(data) - i + 1)
                eta_s = time_to_string(t_eta)
                batch_s = '{:}Case {:03}/{:03} [{:}>{:}] {:} ETA: {:}'.format(
                    init_c + whites, i + 1, len(data),
                    progress_s, remainder_s, time_s, eta_s + '\033[0m'
                )
                print('\033[K', end='', flush=True)
                print(batch_s, end='\r', flush=True)

            if verbose:
                print(
                    '\033[K%sSegmentation finished' % ' '.join([''] * 12)
                )

            seg.append(seg_i)
            unc.append(unc_i)

        return seg, unc
